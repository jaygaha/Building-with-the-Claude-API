{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905bc712",
   "metadata": {},
   "source": [
    "# Exercise on prompt evals\n",
    "\n",
    "## Exercise Task\n",
    "\n",
    "Give the Model Grader more context on what a good solution looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5437be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables and create client\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-haiku-4-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0d8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def add_messages(messages, text, role=\"user\"):\n",
    "    message = {\"role\": role, \"content\": text}\n",
    "    messages.append(message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e788701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a new dataset\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\",\n",
    "        \"solution_criteria\": \"Key criteria for evaluating the solution\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_messages(messages, prompt, role=\"user\")\n",
    "    add_messages(messages, \"```json\", role=\"assistant\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438ed743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and write it to 'dataset.json'\n",
    "dataset = generate_dataset()\n",
    "with open(\"dataset_exercise.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b89174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grade a test case + output using a model\n",
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "Original Task:\n",
    "<task>\n",
    "{test_case[\"task\"]}\n",
    "</task>\n",
    "\n",
    "Solution to Evaluate:\n",
    "<solution>\n",
    "{output}\n",
    "</solution>\n",
    "\n",
    "Criteria you should use to evaluate the solution:\n",
    "<criteria>\n",
    "{test_case[\"solution_criteria\"]}\n",
    "</criteria>\n",
    "\n",
    "\n",
    "Output Format\n",
    "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "- \"strengths\": An array of 1-3 key strengths\n",
    "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "- \"reasoning\": A concise explanation of your overall assessment\n",
    "- \"score\": A number between 1-10\n",
    "\n",
    "Respond with JSON. Keep your response concise and direct.\n",
    "Example response shape:\n",
    "{{\n",
    "    \"strengths\": string[],\n",
    "    \"weaknesses\": string[],\n",
    "    \"reasoning\": string,\n",
    "    \"score\": number\n",
    "}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_messages(messages, eval_prompt, role=\"user\")\n",
    "    add_messages(messages, \"```json\", role=\"assistant\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83809a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passes a test case into Claude\n",
    "def run_prompt(test_case):\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\n",
    "* Respond only with Python, JSON, or a plain Regex\n",
    "* Do not add any comments or commentary or explanation\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_messages(messages, prompt, role=\"user\")\n",
    "    add_messages(messages, \"```code\", role=\"assistant\")\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7953c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to validate the output structure\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bcc4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a single test case and grade the output\n",
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "\n",
    "    score = (model_score + syntax_score) / 2\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa99d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30fae983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 6.666666666666667\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset_exercise.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbcc6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\nimport re\\n\\ndef parse_s3_bucket(uri):\\n    match = re.match(r's3://([a-z0-9.-]+)(/|$)', uri)\\n    return match.group(1) if match else None\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Parse an AWS S3 bucket name from a full S3 URI path (e.g., 's3://my-bucket-name/folder/file.txt') and extract just the bucket name\",\n",
      "      \"format\": \"regex\",\n",
      "      \"solution_criteria\": \"The regex should correctly extract bucket names from S3 URIs, handling buckets with hyphens and numbers, and should not match the path components after the bucket name\"\n",
      "    },\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"The solution effectively handles the core requirement of extracting bucket names while excluding path components. However, it has notable limitations: the lowercase-only character class is unnecessarily restrictive compared to actual S3 bucket naming rules, and there is no validation of bucket name constraints. While these issues may not affect typical well-formed URIs, they represent edge cases and departures from AWS specifications. The solution would benefit from case-insensitive matching and input validation.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\nimport json\\nimport sys\\n\\ndef add_s3_bucket_with_versioning(template):\\n    if isinstance(template, str):\\n        template = json.loads(template)\\n    \\n    if \\\"Resources\\\" not in template:\\n        template[\\\"Resources\\\"] = {}\\n    \\n    template[\\\"Resources\\\"][\\\"S3Bucket\\\"] = {\\n        \\\"Type\\\": \\\"AWS::S3::Bucket\\\",\\n        \\\"Properties\\\": {\\n            \\\"VersioningConfiguration\\\": {\\n                \\\"Status\\\": \\\"Enabled\\\"\\n            }\\n        }\\n    }\\n    \\n    return template\\n\\ninput_data = sys.stdin.read().strip()\\nif input_data:\\n    result = add_s3_bucket_with_versioning(input_data)\\n    print(json.dumps(result, indent=2))\\nelse:\\n    template = {\\n        \\\"AWSTemplateFormatVersion\\\": \\\"2010-09-09\\\",\\n        \\\"Description\\\": \\\"CloudFormation template with S3 bucket\\\",\\n        \\\"Resources\\\": {}\\n    }\\n    result = add_s3_bucket_with_versioning(template)\\n    print(json.dumps(result, indent=2))\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Convert an AWS CloudFormation template JSON object to include a new S3 bucket resource with versioning enabled\",\n",
      "      \"format\": \"json\",\n",
      "      \"solution_criteria\": \"The JSON should be valid CloudFormation syntax, include a new S3 bucket resource with VersioningConfiguration enabled, and maintain the structure of existing resources\"\n",
      "    },\n",
      "    \"score\": 3.5,\n",
      "    \"reasoning\": \"The solution correctly produces valid CloudFormation JSON with a properly configured S3 bucket resource including versioning. The core functionality works as specified and maintains template structure integrity. However, the implementation lacks flexibility (hard-coded resource name), robust error handling, and input validation. For production use, these gaps should be addressed, though for a simple conversion task the solution is functional.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\nimport json\\nfrom typing import List, Dict, Union, Any\\n\\ndef extract_iam_actions(policy_document: Dict[str, Any]) -> List[str]:\\n    \\\"\\\"\\\"\\n    Extract all Action permissions from an AWS IAM policy document.\\n    \\n    Args:\\n        policy_document: AWS IAM policy as a dictionary\\n        \\n    Returns:\\n        List of all Action permissions granted in the policy\\n    \\\"\\\"\\\"\\n    actions = []\\n    \\n    if not isinstance(policy_document, dict):\\n        return actions\\n    \\n    statements = policy_document.get('Statement', [])\\n    \\n    if not isinstance(statements, list):\\n        statements = [statements]\\n    \\n    for statement in statements:\\n        if not isinstance(statement, dict):\\n            continue\\n        \\n        if statement.get('Effect') != 'Allow':\\n            continue\\n        \\n        action_value = statement.get('Action')\\n        \\n        if action_value is None:\\n            continue\\n        \\n        if isinstance(action_value, str):\\n            actions.append(action_value)\\n        elif isinstance(action_value, list):\\n            for action in action_value:\\n                if isinstance(action, str):\\n                    actions.append(action)\\n    \\n    return actions\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function that takes an AWS IAM policy document as a dictionary and returns a list of all Action permissions granted in the policy\",\n",
      "      \"format\": \"python\",\n",
      "      \"solution_criteria\": \"The function should handle both string and list values for Action fields, extract all unique actions across all statements, and return them as a flat list\"\n",
      "    },\n",
      "    \"score\": 8.0,\n",
      "    \"reasoning\": \"The solution demonstrates solid defensive programming and correctly handles the structural parsing of IAM policies. However, it has a critical flaw: the criteria explicitly requires 'extract all unique actions' but the function returns a list without deduplication. Adding a single line (converting to set then back to list) would fix this. The Effect='Allow' filter is also questionable\\u2014while common, the task doesn't specify filtering by effect, and some use cases may need all actions regardless. The function works for basic cases but doesn't fully meet the stated requirements.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
